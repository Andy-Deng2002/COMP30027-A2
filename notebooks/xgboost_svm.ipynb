{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gc\n",
    "import os \n",
    "import cv2\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the availability of GPU on xgboost\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "params = {\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"verbosity\": 1\n",
    "}\n",
    "\n",
    "try:\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=10)\n",
    "    print(\"XGBoost GPU is available and working!\")\n",
    "except xgb.core.XGBoostError as e:\n",
    "    print(\"XGBoost GPU is NOT available! Change the tree_method to cpu_hist in cell 27\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the file path to your own path\n",
    "file_path = r\"C:\\Users\\xiluo\\Desktop\\UoM 2025 S1\\ML\\COMP30027 asmt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the train dataframes\n",
    "train_metadata = pd.read_csv(os.path.join(file_path, \"data\", \"train\", \"train_metadata.csv\"))\n",
    "train_color_features = pd.read_csv(os.path.join(file_path, \"data\", \"train\", \"Features\", \"color_histogram.csv\"))\n",
    "train_pca_features = pd.read_csv(os.path.join(file_path, \"data\", \"train\", \"Features\", \"hog_pca.csv\"))\n",
    "train_additional_features = pd.read_csv(os.path.join(file_path, \"data\", \"train\", \"Features\", \"additional_features.csv\"))\n",
    "\n",
    "train_metadata = train_metadata.merge(train_color_features, on=\"image_path\", how=\"left\")\n",
    "train_metadata = train_metadata.merge(train_pca_features, on=\"image_path\", how=\"left\")\n",
    "train_metadata = train_metadata.merge(train_additional_features, on=\"image_path\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the test dataframes\n",
    "test_metadata = pd.read_csv(os.path.join(file_path, \"data\", \"test\", \"test_metadata.csv\"))\n",
    "test_color_features = pd.read_csv(os.path.join(file_path, \"data\", \"test\", \"Features\", \"color_histogram.csv\"))\n",
    "test_pca_features = pd.read_csv(os.path.join(file_path, \"data\", \"test\", \"Features\", \"hog_pca.csv\"))\n",
    "test_additional_features = pd.read_csv(os.path.join(file_path, \"data\", \"test\", \"Features\", \"additional_features.csv\"))    \n",
    "\n",
    "test_metadata = test_metadata.merge(test_color_features, on=\"image_path\", how=\"left\")\n",
    "test_metadata = test_metadata.merge(test_pca_features, on=\"image_path\", how=\"left\")\n",
    "test_metadata = test_metadata.merge(test_additional_features, on=\"image_path\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in train_metadata.columns if col not in [\"image_path\", \"id\", \"ClassId\"]]\n",
    "print(len(features))\n",
    "\n",
    "# First split for train set and holdout set\n",
    "trainset, holdout_set = train_test_split(\n",
    "    train_metadata,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_metadata['ClassId'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train = trainset[features]\n",
    "y_train = trainset[\"ClassId\"]\n",
    "X_test = test_metadata[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Stratified Cross Validation for training\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialise arrays and list to save training results\n",
    "xgb_val_preds = np.zeros(X_train.shape[0])\n",
    "xgb_test_preds = np.zeros((X_test.shape[0], 43, n_splits))\n",
    "xgb_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    # get the sub training set and val set in each loop\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    xgb_params = {\n",
    "    'objective': 'multi:logloss',\n",
    "    'num_class': 43,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 1000,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 3,\n",
    "    'alpha': 0.1,\n",
    "    'lambda': 0.1,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0,\n",
    "    'tree_method': 'gpu_hist', # change to gpu_hist if gpu is available\n",
    "    'early_stopping_rounds': 50,\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=100)\n",
    "    \n",
    "    # Get the OOF prediction as validation result and for error analysis\n",
    "    xgb_val_preds[val_idx] = model.predict(X_val)\n",
    "\n",
    "    # Get the predicted probabilities on test set in each loop and aggregate later for robustness\n",
    "    xgb_test_preds[:, :, fold] = model.predict_proba(X_test)\n",
    "\n",
    "    # Save the model \n",
    "    xgb_models.append(model)\n",
    "\n",
    "val_acc = accuracy_score(y_train, xgb_val_preds)\n",
    "print(f\"Validation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "macro_f1 = f1_score(y_train, xgb_val_preds, average='macro')\n",
    "print(f\"Macro F1 Score: {macro_f1:.5f}\")\n",
    "\n",
    "# save validation results for visualisation\n",
    "save_dir = os.path.join(file_path, \"results\", \"sankey_data\")\n",
    "np.save(os.path.join(save_dir, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(save_dir, \"xgb_val_pred_labels.npy\"), xgb_val_preds)\n",
    "\n",
    "# Save the 5 xgb models\n",
    "with open(os.path.join(file_path, \"models\", \"xgb_models.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(xgb_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the 5 models with different training subsets\n",
    "xgb_test_pred_probs = xgb_test_preds.mean(axis=2)  # shape: (n_test, n_classes)\n",
    "\n",
    "# Select the class with highest probability\n",
    "xgb_test_pred_labels = xgb_test_pred_probs.argmax(axis=1)  # shape: (n_test,)\n",
    "\n",
    "# Save to result as the prediction for single XGB model\n",
    "test_metadata[\"ClassId\"] = xgb_test_pred_labels.astype(int)\n",
    "test_metadata[[\"id\", \"ClassId\"]].to_csv(os.path.join(file_path, \"results\", \"submission_xgb.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dict = classification_report(y_train, xgb_val_preds, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "print(report_df)\n",
    "\n",
    "# Remove last three rows\n",
    "class_rows = report_df.iloc[:-3, :]\n",
    "\n",
    "x = list(range(43)) # The class labels\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(x, class_rows['precision'], marker='o', label='Precision', color='#1f77b4')\n",
    "plt.plot(x, class_rows['recall'], marker='o', label='Recall', color='#2ca02c')\n",
    "plt.plot(x, class_rows['f1-score'], marker='o', label='F1-score', color='#ff7f0e')\n",
    "\n",
    "plt.axhline(report_df.loc['macro avg', 'f1-score'], color='gray', linestyle='--', label='Macro F1')\n",
    "plt.axhline(report_df.loc['weighted avg', 'f1-score'], color='orange', linestyle='--', label='Weighted F1')\n",
    "\n",
    "plt.xlabel('Class', fontsize=13)\n",
    "plt.ylabel('Score', fontsize=13)\n",
    "plt.title('XGBoost Per-Class Precision, Recall, F1-score (with Macro/Weighted F1)', fontsize=15, pad=12)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xticks(x, x, fontsize=11, rotation=0)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.legend(loc='lower left', fontsize=11, framealpha=0.85)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shape_based_misclassifications(y_true, y_pred, classes, class_title, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    draw the shape distribution of the misclassified samples for certain classes\n",
    "    \"\"\"\n",
    "    \n",
    "    # classify shapes and colors\n",
    "    shape_classification = {\n",
    "        'red_white_circular': [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 15, 16],\n",
    "        'red_white_triangular': [11, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
    "        'black_white_circular': [6, 32, 41, 42],\n",
    "        'other': [12, 13, 14, 17],\n",
    "        'blue_arrow_circular': [33, 34, 35, 36, 37, 38, 39, 40],\n",
    "    }\n",
    "    \n",
    "    shape_colors = {\n",
    "        'red_white_circular': '#FF0000',\n",
    "        'red_white_triangular': '#FF9999',\n",
    "        'black_white_circular': '#808080',\n",
    "        'other': '#2ECC71',\n",
    "        'blue_arrow_circular': '#3498DB'\n",
    "    }\n",
    "    \n",
    "    def get_shape_category(class_id):\n",
    "        class_id = int(class_id)\n",
    "        for shape, classes in shape_classification.items():\n",
    "            if class_id in classes:\n",
    "                return shape\n",
    "        return 'unknown'\n",
    "    \n",
    "    if classes is None:\n",
    "        classes = range(len(np.unique(y_true)))\n",
    "    \n",
    "    # gather misclassified info\n",
    "    error_data = {}\n",
    "    for cls in classes:\n",
    "        mask = y_true == cls\n",
    "        predictions = y_pred[mask]\n",
    "        unique_preds, pred_counts = np.unique(predictions, return_counts=True)\n",
    "        error_dict = {pred: count for pred, count in zip(unique_preds, pred_counts) if pred != cls}\n",
    "        error_data[cls] = error_dict\n",
    "    \n",
    "    # count and classify in terms of shape\n",
    "    shape_errors = {shape: np.zeros(len(classes)) for shape in shape_classification.keys()}\n",
    "    for i, cls in enumerate(classes):\n",
    "        for pred_cls, count in error_data[cls].items():\n",
    "            pred_shape = get_shape_category(pred_cls)\n",
    "            if pred_shape in shape_errors:\n",
    "                shape_errors[pred_shape][i] += count\n",
    "    unique_class_count = {cls: len(error_data[cls].keys()) for cls in classes}\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    bottom = np.zeros(len(classes))\n",
    "    \n",
    "    # draw stacked barplots\n",
    "    for shape in shape_classification.keys():\n",
    "        if np.any(shape_errors[shape]):\n",
    "            plt.bar(range(len(classes)), shape_errors[shape], \n",
    "                    bottom=bottom, label=shape, color=shape_colors[shape])\n",
    "            bottom += shape_errors[shape]\n",
    "    \n",
    "    # add values on bars\n",
    "    for i, total in enumerate(bottom):\n",
    "        if total > 0:\n",
    "            cls = list(classes)[i]\n",
    "            plt.text(i, total * 1.02,\n",
    "                    f'{int(total)}\\n({unique_class_count[cls]} classes)', \n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    plt.xlabel('Original Class', fontsize=12)\n",
    "    plt.ylabel('Number of Misclassifications', fontsize=12)\n",
    "    title = 'Distribution of Shape-based Misclassifications'\n",
    "    title += class_title\n",
    "    plt.title(title, fontsize=14)\n",
    "    x_labels = [f'Class {cls}\\n({get_shape_category(cls)})' for cls in classes]\n",
    "    plt.xticks(range(len(classes)), x_labels, rotation=45, ha='right')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.ylim(0, max(bottom) * 1.1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# get the class with top 10 lowest f1-score\n",
    "report = classification_report(y_train, xgb_val_preds, output_dict=True)\n",
    "f1_scores = [report[str(i)]['f1-score'] for i in range(43)]\n",
    "lowest_10_classes = np.argsort(f1_scores)[:10]\n",
    "plot_shape_based_misclassifications(y_train, xgb_val_preds, lowest_10_classes, 'for 10 lowest F1-score classes', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shape_based_misclassifications(y_train, xgb_val_preds, range(32,43), 'for uncommon classes', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_stats(val_df, y_val, y_pred, cls_A, cls_B, features):\n",
    "    \"\"\"\n",
    "    Compute mean and std for each feature in:\n",
    "    - cls_A correctly predicted as cls_A\n",
    "    - cls_B correctly predicted as cls_B\n",
    "    - cls_A misclassified as cls_B\n",
    "    \"\"\"\n",
    "    idx_AA = np.where((y_val == cls_A) & (y_pred == cls_A))[0]\n",
    "    idx_BB = np.where((y_val == cls_B) & (y_pred == cls_B))[0]\n",
    "    idx_AB = np.where((y_val == cls_A) & (y_pred == cls_B))[0]\n",
    "    stats = []\n",
    "    for feat in features:\n",
    "        mean_AA = val_df.iloc[idx_AA][feat].mean()\n",
    "        std_AA = val_df.iloc[idx_AA][feat].std()\n",
    "        mean_BB = val_df.iloc[idx_BB][feat].mean()\n",
    "        std_BB = val_df.iloc[idx_BB][feat].std()\n",
    "        mean_AB = val_df.iloc[idx_AB][feat].mean()\n",
    "        std_AB = val_df.iloc[idx_AB][feat].std()\n",
    "        stats.append({\n",
    "            'feature': feat,\n",
    "            f'{cls_A}->{cls_A} mean': mean_AA,\n",
    "            f'{cls_A}->{cls_A} std': std_AA,\n",
    "            f'{cls_B}->{cls_B} mean': mean_BB,\n",
    "            f'{cls_B}->{cls_B} std': std_BB,\n",
    "            f'{cls_A}->{cls_B} mean': mean_AB,\n",
    "            f'{cls_A}->{cls_B} std': std_AB,\n",
    "        })\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "def plot_top_features(stats_df, title):\n",
    "    \"\"\"\n",
    "    Plot the top 20 features with the largest difference ratio.\n",
    "    \"\"\"\n",
    "    top_20_features = stats_df.nlargest(20, 'diff_ratio')\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    bars = plt.barh(top_20_features['feature'], top_20_features['diff_ratio'])\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{width:.2f}', \n",
    "                 ha='left', va='center', fontsize=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Difference Ratio')\n",
    "    plt.ylabel('Feature Name')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get per-class F1 scores\n",
    "report = classification_report(y_train, xgb_val_preds, output_dict=True)\n",
    "f1_scores = [report[str(i)]['f1-score'] for i in range(43)]\n",
    "\n",
    "# Find the class with lowest and highest F1-score\n",
    "worst_cls = np.argmin(f1_scores)\n",
    "best_cls = np.argmax(f1_scores)\n",
    "\n",
    "print(f\"Class with lowest F1-score: {worst_cls} (F1 = {f1_scores[worst_cls]:.4f})\")\n",
    "print(f\"Class with highest F1-score: {best_cls} (F1 = {f1_scores[best_cls]:.4f})\")\n",
    "\n",
    "# For the worst class, find which class it is most often confused with\n",
    "cm = confusion_matrix(y_train, xgb_val_preds, labels=np.arange(43))\n",
    "pred_counts = cm[worst_cls].copy()\n",
    "pred_counts[worst_cls] = 0  # Exclude correct predictions\n",
    "most_confused = np.argmax(pred_counts)\n",
    "print(f\"\\nClass {worst_cls} is most often predicted as: {most_confused}\")\n",
    "print(f\"Number of misclassifications: {pred_counts[most_confused]}\")\n",
    "\n",
    "# Analyze feature differences\n",
    "features_to_analyze = X_train.columns.tolist()\n",
    "\n",
    "# Worst class vs its most confused class\n",
    "print(\"\\nAnalyzing worst performing class vs its most confused class:\")\n",
    "stats_df_worst = feature_stats(X_train, y_train, xgb_val_preds, worst_cls, most_confused, features_to_analyze)\n",
    "stats_df_worst['diff_ratio'] = abs(stats_df_worst[f'{worst_cls}->{worst_cls} mean'] - stats_df_worst[f'{most_confused}->{most_confused} mean']) / \\\n",
    "                              ((stats_df_worst[f'{worst_cls}->{worst_cls} std'] + stats_df_worst[f'{most_confused}->{most_confused} std']) / 2)\n",
    "\n",
    "# Plot results\n",
    "plot_top_features(stats_df_worst, f'Top 20 Most Discriminative Features Between Class {worst_cls} and {most_confused} (Most Confused)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_class_samples(class_ids):\n",
    "    \"\"\"\n",
    "    show specified class samples\n",
    "    \"\"\"\n",
    "    n_classes = len(class_ids)\n",
    "    fig, axes = plt.subplots(1, n_classes, figsize=(8, 4))\n",
    "    \n",
    "    for i, class_id in enumerate(class_ids):\n",
    "        # get all image paths of the class\n",
    "        class_samples = train_metadata[train_metadata['ClassId'] == class_id]['image_path'].values\n",
    "        \n",
    "        # randomly select n_samples samples\n",
    "        selected_samples = np.random.choice(class_samples, 1, replace=False)\n",
    "        \n",
    "        for j, img_path in enumerate(selected_samples):\n",
    "            # read and show images\n",
    "            img_path = os.path.join(file_path, \"data\", \"train\", img_path)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f'Class {class_id}', pad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# show the samples of class 0 and 1\n",
    "show_class_samples([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the shap values for each model and average them over samples and classes\n",
    "shap_importances = []\n",
    "for model in xgb_models:\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train.sample(n=1000, random_state=42))\n",
    "    # Average the shap values over samples and classes\n",
    "    mean_shap = np.abs(shap_values).mean(axis=(0, 2))\n",
    "    shap_importances.append(mean_shap)\n",
    "\n",
    "# Average the shap values over 5 folds\n",
    "mean_shap_importance = np.mean(shap_importances, axis=0)\n",
    "feature_importance = pd.Series(mean_shap_importance, index=X_train.columns).sort_values(ascending=False)\n",
    "print(feature_importance.head(30))\n",
    "\n",
    "topn = 30\n",
    "top_features = feature_importance.head(topn)[::-1]\n",
    "plt.figure(figsize=(10, 8))\n",
    "bars = plt.barh(top_features.index, top_features.values, color=\"#4682b4\")\n",
    "plt.xlabel(\"Mean(|SHAP value|)\", fontsize=13)\n",
    "plt.ylabel(\"Feature\", fontsize=13)\n",
    "plt.title(\"Top 30 Feature Importances (Mean SHAP, 5-fold XGBoost)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "n_classes = 43\n",
    "svm_val_preds = np.zeros(X_train.shape[0], dtype=int)\n",
    "svm_val_probs = np.zeros((X_train.shape[0], n_classes))\n",
    "svm_test_preds = np.zeros((X_test.shape[0], n_classes, n_splits))\n",
    "svm_models = []\n",
    "scalers = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = SVC(probability=True, random_state=42, C=10, gamma='scale', kernel='rbf')\n",
    "    model.fit(X_tr_scaled, y_tr)\n",
    "    \n",
    "    # OOF prediction\n",
    "    val_pred = model.predict(X_val_scaled)\n",
    "    val_prob = model.predict_proba(X_val_scaled)\n",
    "    svm_val_preds[val_idx] = val_pred\n",
    "    svm_val_probs[val_idx] = val_prob\n",
    "        \n",
    "    # Test prediction\n",
    "    svm_test_preds[:, :, fold] = model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    svm_models.append(model)\n",
    "    scalers.append(scaler)\n",
    "\n",
    "# validation accuracy\n",
    "val_acc = accuracy_score(y_train, svm_val_preds)\n",
    "print(f\"Validation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "# save the model and validation results\n",
    "np.save(os.path.join(save_dir, \"svm_val_pred_labels.npy\"), svm_val_preds)\n",
    "with open(os.path.join(file_path, \"models\", \"svm_models.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(svm_models, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dict = classification_report(y_train, svm_val_preds, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "print(report_df)\n",
    "\n",
    "# Only keep the class rows (remove accuracy, macro avg, weighted avg)\n",
    "class_rows = report_df.iloc[:-3, :]\n",
    "\n",
    "x = list(range(43))  # The class labels\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(x, class_rows['precision'], marker='o', label='Precision', color='#1f77b4')\n",
    "plt.plot(x, class_rows['recall'], marker='o', label='Recall', color='#2ca02c')\n",
    "plt.plot(x, class_rows['f1-score'], marker='o', label='F1-score', color='#ff7f0e')\n",
    "\n",
    "plt.axhline(report_df.loc['macro avg', 'f1-score'], color='gray', linestyle='--', label='Macro F1')\n",
    "plt.axhline(report_df.loc['weighted avg', 'f1-score'], color='orange', linestyle='--', label='Weighted F1')\n",
    "\n",
    "plt.xlabel('Class', fontsize=13)\n",
    "plt.ylabel('Score', fontsize=13)\n",
    "plt.title('SVM Per-Class Precision, Recall, F1-score (with Macro/Weighted F1)', fontsize=15, pad=12)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xticks(x, x, fontsize=11, rotation=0)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.legend(loc='lower left', fontsize=11, framealpha=0.85)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_train, svm_val_preds, output_dict=True)\n",
    "f1_scores = [report[str(i)]['f1-score'] for i in range(n_classes)]\n",
    "lowest_10_classes = np.argsort(f1_scores)[:10]\n",
    "print(\"Lowest 10 F1-score classes:\", lowest_10_classes)\n",
    "\n",
    "plot_shape_based_misclassifications(y_train, svm_val_preds, lowest_10_classes, 'for 10 lowest F1-score classes', figsize=(12, 8))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shape_based_misclassifications(y_train, svm_val_preds, range(32,43), 'for uncommon classes', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_classes = 43 \n",
    "gamma = svm_models[0]._gamma\n",
    "\n",
    "# initialize the storage of three similarities\n",
    "class_correct_to_sv = [[] for _ in range(n_classes)]      # correct samples to class support vectors\n",
    "class_wrong_to_sv = [[] for _ in range(n_classes)]        # wrong samples to class support vectors\n",
    "class_wrong_to_correct = [[] for _ in range(n_classes)]   # wrong samples to correct samples\n",
    "\n",
    "def rbf_kernel(X, Y, gamma):\n",
    "    dists = np.sum((X[:, None, :] - Y[None, :, :]) ** 2, axis=2)\n",
    "    return np.exp(-gamma * dists)\n",
    "\n",
    "def mean_without_outliers(arr, lower=0.05, upper=0.95):\n",
    "    arr = np.asarray(arr)\n",
    "    if len(arr) == 0:\n",
    "        return np.nan\n",
    "    q_low = np.quantile(arr, lower)\n",
    "    q_high = np.quantile(arr, upper)\n",
    "    filtered = arr[(arr >= q_low) & (arr <= q_high)]\n",
    "    return filtered.mean() if len(filtered) > 0 else np.median(arr)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    model = svm_models[fold]\n",
    "    scaler = scalers[fold]\n",
    "    X_val = X_train.iloc[val_idx]\n",
    "    y_val = y_train.iloc[val_idx].values\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    # get the class label of support vectors\n",
    "    sv = model.support_vectors_\n",
    "    sv_labels = model.predict(sv)\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        mask_cls = (y_val == cls)\n",
    "        if not np.any(mask_cls):\n",
    "            continue\n",
    "            \n",
    "        # get correct and wrong samples\n",
    "        mask_correct = mask_cls & (y_val == y_val_pred)\n",
    "        mask_wrong = mask_cls & (y_val != y_val_pred)\n",
    "        X_correct = X_val_scaled[mask_correct]\n",
    "        X_wrong = X_val_scaled[mask_wrong]\n",
    "        \n",
    "        # get the support vectors of current class\n",
    "        sv_cls_mask = (sv_labels == cls)\n",
    "        sv_cls = sv[sv_cls_mask]\n",
    "        \n",
    "        if len(sv_cls) == 0:\n",
    "            continue\n",
    "\n",
    "        if len(X_correct) > 0:\n",
    "            # calculate the similarity between correct samples and class support vectors\n",
    "            K_correct_sv = rbf_kernel(X_correct, sv_cls, gamma)\n",
    "            class_correct_to_sv[cls].extend(K_correct_sv.max(axis=1))\n",
    "\n",
    "        if len(X_wrong) > 0:\n",
    "            # calculate the similarity between wrong samples and class support vectors\n",
    "            K_wrong_sv = rbf_kernel(X_wrong, sv_cls, gamma)\n",
    "            class_wrong_to_sv[cls].extend(K_wrong_sv.max(axis=1))\n",
    "            \n",
    "            # calculate the similarity between wrong samples and correct samples\n",
    "            if len(X_correct) > 0:\n",
    "                K_wrong_correct = rbf_kernel(X_wrong, X_correct, gamma)\n",
    "                class_wrong_to_correct[cls].extend(K_wrong_correct.max(axis=1))\n",
    "\n",
    "# calculate the average similarity\n",
    "class_correct_sv_mean = [mean_without_outliers(v) for v in class_correct_to_sv]\n",
    "class_wrong_sv_mean = [mean_without_outliers(v) for v in class_wrong_to_sv]\n",
    "class_wrong_correct_mean = [mean_without_outliers(v) for v in class_wrong_to_correct]\n",
    "\n",
    "# visualize\n",
    "x = np.arange(n_classes)\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(x, class_correct_sv_mean, marker='o', label='Correct to Class SV', color='green')\n",
    "plt.plot(x, class_wrong_sv_mean, marker='x', label='Wrong to Class SV', color='red')\n",
    "plt.plot(x, class_wrong_correct_mean, marker='^', label='Wrong to Correct', color='blue')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Mean Max RBF Kernel Value')\n",
    "plt.title('Sample Similarities Comparison (Using Class-Specific Support Vectors)')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the 5 models with different training subsets\n",
    "svm_test_pred_probs = svm_test_preds.mean(axis=2)  # shape: (n_test, n_classes)\n",
    "\n",
    "# Select the class with highest probability\n",
    "svm_test_pred_labels = svm_test_pred_probs.argmax(axis=1)  # shape: (n_test,)\n",
    "\n",
    "# Save the prediction as the submission of single SVM model\n",
    "test_metadata[\"ClassId\"] = svm_test_pred_labels.astype(int)\n",
    "test_metadata[[\"id\", \"ClassId\"]].to_csv(os.path.join(file_path, \"results\", \"submission_svm.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Holdout Set Inference & Evaluation\n",
    "n_holdout = len(holdout_set)\n",
    "n_classes = 43\n",
    "xgb_holdout_probs_folds = np.zeros((n_holdout, n_classes, 5))\n",
    "\n",
    "for fold in range(5):\n",
    "    xgb_model = xgb_models[fold]\n",
    "    holdout_xgb_feats = holdout_set[features]\n",
    "    xgb_holdout_probs = xgb_model.predict_proba(holdout_xgb_feats)\n",
    "    xgb_holdout_probs_folds[:, :, fold] = xgb_holdout_probs\n",
    "\n",
    "# average 5 models in 5 folds\n",
    "xgb_holdout_probs_mean = np.mean(xgb_holdout_probs_folds, axis=2)\n",
    "xgb_holdout_preds = np.argmax(xgb_holdout_probs_mean, axis=1)\n",
    "np.save(os.path.join(save_dir, \"xgb_holdout_pred_labels.npy\"), xgb_holdout_preds)\n",
    "\n",
    "# evaluate\n",
    "y_true = holdout_set['ClassId']\n",
    "report_dict = classification_report(y_true, xgb_holdout_preds, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "print(report_df)\n",
    "\n",
    "# visualise\n",
    "class_rows = report_df.iloc[:-3, :]\n",
    "x = list(range(n_classes))\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(x, class_rows['precision'], marker='o', label='Precision', color='#1f77b4')\n",
    "plt.plot(x, class_rows['recall'], marker='o', label='Recall', color='#2ca02c')\n",
    "plt.plot(x, class_rows['f1-score'], marker='o', label='F1-score', color='#ff7f0e')\n",
    "plt.axhline(report_df.loc['macro avg', 'f1-score'], color='gray', linestyle='--', label='Macro F1')\n",
    "plt.axhline(report_df.loc['weighted avg', 'f1-score'], color='orange', linestyle='--', label='Weighted F1')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Score')\n",
    "plt.title('XGBoost Per-Class Precision, Recall, F1-score (with Macro/Weighted F1)')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xticks(x, x)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Holdout Set Inference & Evaluation\n",
    "n_holdout = len(holdout_set)\n",
    "n_classes = 43\n",
    "svm_holdout_probs_folds = np.zeros((n_holdout, n_classes, 5))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_metadata, train_metadata['ClassId'])):\n",
    "    svm_model = svm_models[fold]\n",
    "    scaler = StandardScaler()\n",
    "    train_svm_feats = train_metadata.iloc[train_idx][features]\n",
    "    scaler.fit(train_svm_feats)\n",
    "    holdout_svm_feats = holdout_set[features]\n",
    "    holdout_svm_feats_scaled = scaler.transform(holdout_svm_feats)\n",
    "    svm_holdout_probs = svm_model.predict_proba(holdout_svm_feats_scaled)\n",
    "    svm_holdout_probs_folds[:, :, fold] = svm_holdout_probs\n",
    "\n",
    "# average 5 models\n",
    "svm_holdout_probs_mean = np.mean(svm_holdout_probs_folds, axis=2)\n",
    "svm_holdout_preds = np.argmax(svm_holdout_probs_mean, axis=1)\n",
    "np.save(os.path.join(save_dir, \"svm_holdout_pred_labels.npy\"), svm_holdout_preds)\n",
    "\n",
    "# evaluate \n",
    "y_true = holdout_set['ClassId']\n",
    "report_dict = classification_report(y_true, svm_holdout_preds, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "print(report_df)\n",
    "\n",
    "# visualise results\n",
    "class_rows = report_df.iloc[:-3, :]\n",
    "x = list(range(n_classes))\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(x, class_rows['precision'], marker='o', label='Precision', color='#1f77b4')\n",
    "plt.plot(x, class_rows['recall'], marker='o', label='Recall', color='#2ca02c')\n",
    "plt.plot(x, class_rows['f1-score'], marker='o', label='F1-score', color='#ff7f0e')\n",
    "plt.axhline(report_df.loc['macro avg', 'f1-score'], color='gray', linestyle='--', label='Macro F1')\n",
    "plt.axhline(report_df.loc['weighted avg', 'f1-score'], color='orange', linestyle='--', label='Weighted F1')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Score')\n",
    "plt.title('SVM Per-Class Precision, Recall, F1-score (with Macro/Weighted F1)')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xticks(x, x)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP30027_A2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
